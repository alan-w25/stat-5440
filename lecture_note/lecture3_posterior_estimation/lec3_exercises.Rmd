<h1>In Class Exercise Lecture 3</h1> 

### imports 

```{r}
library(tidyverse)
library(ggplot2)
library(bayesrules)
library(HDInterval)
library(palmerpenguins)
library(dplyr)
```

Exercise / Example 1: MTCars and posterior estimation using intervals 

```{r}
cars <- mtcars 
y <- cars$mpg 
n <- length(y)
y_bar <- mean(y)
head(cars)
```
```{r}
hist(y)
```

## Task 1: Analyze mean mpg  

- Assume normal prior on mu with prior mean $m_\mu = 22$ and prior variance $s_{\mu}^2 = 25$
- Likelihood: $y \sim N(\mu, \sigma^2)$ with known variance $\sigma^2 = 9$

Remember the formula for normal-normal conjugacy with known variance is the following: 

1. the posterior mean is a weighted sum of the inverses of the data and the prior variances
2. the posterior variance is the inverse of the sum of the prior and the data variance

expected: (m_star, s2_star) = 20.11187, 0.2781211

```{r}

m_mu <- 22 
s2_mu <- 25 
sigma2 <- 9

s2_star <- 1/((1/s2_mu) + (n/sigma2))
m_star <- ((1/s2_mu) / (n/sigma2 + 1/s2_mu))*m_mu + 
  ((n/sigma2)/(n/sigma2 + 1/s2_mu))*y_bar 

m_star 
s2_star
```

We first generate the central posterior credible interval of 95% 
```{r}
#generate posterior sample then 
posterior <- rnorm(10000, m_star, sqrt(s2_star))


#analytical 
lo <- qnorm(0.025, mean = m_star, sd = sqrt(s2_star))
hi <- qnorm(0.975, mean = m_star, sd = sqrt(s2_star)) 

cat("95% credible interval generated by analytical : [", lo, ", ", hi, "]\n")

#sample
print("The posterior credible interval generated by sampling the posterior distribution")
quantile(posterior, c(0.025, 0.975))
```
We can generate another 95% credible interval, but just not the central one 

```{r}
print("analytical 95% credible interval from 0.01 to 0.96") 
qnorm(c(0.01, 0.96), mean = m_star, sd = sqrt(s2_star))

print("sample 95% credible interval from 0.01 to 0.96")
quantile(posterior, c(0.01, 0.96))

```

Now compute the HDPI for the same posterior distribution (the expectation is that it will be similar if not the same as central credible interval)
```{r}
hpdi <- hdi(posterior, credMass = 0.95)
print("HDPI")
hpdi
```

## Task 2: Bayesian Hypothesis Testing

H0: mu <= 20 
H1: mu > 20 

```{r}
ph1 <- mean(posterior > 20)

cat("Posterior Mean:", m_star, "\n")
cat("P(mu > 20):", ph1, "\n")

```

## Task 3: Posterior Predictive 

So the posterior predictive distribution gives us a way to validate our models and also predict the correct distribution from which our data comes from. For this task, we will be predicting bill lengths of penguins 

The assumptions: 

1. normal normal 
2. prior: mean = 50, s2_mu = 100 
3. sigma2 = 9 

```{r}
penguin <- penguins |> 
  filter(!is.na(bill_length_mm))

hist(penguin$bill_length_mm)
head(penguin)
```
Expected (m_star, s2_star) = (43.92353, 0.02630887)
```{r}
y <- penguin$bill_length_mm 
y_bar <- mean(y) 
n <- nrow(penguin)

m_mu <- 50 
s2_mu <- 100 
sigma2 <- 9 

s2_star <- 1/((1/s2_mu) + (n/sigma2))
m_star <- ((1/s2_mu) / (n/sigma2 + 1/s2_mu))*m_mu + 
  ((n/sigma2)/(n/sigma2 + 1/s2_mu))*y_bar 

m_star
s2_star
```

We plot the posterior distribution and notice that the mean is roughly arouind 43.9
```{r}
posterior <- rnorm(10000, mean=m_star, sd = sqrt(s2_star))
hist(posterior, freq = FALSE)
lines(density(posterior), col = 'red')
```

We construct the posterior predictive distribution from samples of the posterior (so all realizations)

```{r}
posterior_predictive <- rnorm(10000, mean = posterior, sd = sqrt(sigma2))
hist(posterior_predictive, main = "Posterior Predictive for Bill Length", xlab = "Bill Length (mm)")

```
Now in closed form: 

```{r}
y_pred <- rnorm(10000, mean=m_star, sd = sqrt(s2_star + sigma2))
hist(y_pred, main = "Posterior Predictive (closed form) for Bill Length", xlab = "Bill Length (mm)")

```